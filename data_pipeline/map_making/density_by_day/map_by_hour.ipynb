{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import media_mapper as mm\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##gather data and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_and_merge_tweet_data():\n",
    "    '''Retrieves twitter geo data from SQL, and tweet text. \n",
    "    Returns the merged dataframe.'''\n",
    "    #get SF Data From SQL\n",
    "    df = mm.pipeline.retrieve_sql_tweets('tweets_with_geoV6')\n",
    "    #get text data from picke\n",
    "    dftxt = pd.read_csv('../../data/intermediate_data/json_tweets_in_df_twitokend.csv')\n",
    "    df = df.set_index('id')\n",
    "    dftxt = dftxt.set_index('id')\n",
    "    dfall = df.join(dftxt).reset_index()\n",
    "    dfall.drop('Unnamed: 0', 1, inplace = True)\n",
    "    return dfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = retrieve_and_merge_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###break data into hours and regions. add tweet corpus to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_df = mm.pipeline.transform_timestamp(df, hour = True)\n",
    "hour_df['tweetcnt'] = 1\n",
    "#get a total count of tweets\n",
    "hdf = hour_df.groupby(['geoid10', 'hour']).agg(sum).reset_index().drop('id', 1)\n",
    "#get a grouped sum of the words\n",
    "hour_df_txt = hour_df.groupby(['geoid10', 'hour'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "hdf['tokens'] = hour_df_txt['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>hour</th>\n",
       "      <th>tweetcnt</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[u'#ship', u'#photo', u'#edit', u'#sail', u'@'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[u'My', u'lovey', u'@', u'Danville', u',', u'C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[u'What', u'up', u',', u'San', u'Francisco', u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[u'The', u'#rough', u'#wild', u'#choppy', u'#p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[u\"I'm\", u'at', u\"Scoma's\", u'Restaurant', u'-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10  hour  tweetcnt  \\\n",
       "0  060750101001000     0         4   \n",
       "1  060750101001000     1         4   \n",
       "2  060750101001000     2         3   \n",
       "3  060750101001000     3         8   \n",
       "4  060750101001000     4         2   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'#ship', u'#photo', u'#edit', u'#sail', u'@'...  \n",
       "1  [u'My', u'lovey', u'@', u'Danville', u',', u'C...  \n",
       "2  [u'What', u'up', u',', u'San', u'Francisco', u...  \n",
       "3  [u'The', u'#rough', u'#wild', u'#choppy', u'#p...  \n",
       "4  [u\"I'm\", u'at', u\"Scoma's\", u'Restaurant', u'-...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###get top ten tokens as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend([',', '-', ',', '?', '@', '&', ')', '(', ';','.',':','!','..','...'] )\n",
    "\n",
    "def top_tokens(corpus_list, stopwords= stopwords, number=10):\n",
    "    '''Takes a list of tokens. Returns the top ten, unless a different number given.'''\n",
    "    #Checks to make sure the tokens are in a list, and not a string\n",
    "    tokens = literal_eval(corpus_list)\n",
    "    #If there are multiple tweets, flatten the list\n",
    "    if type(tokens) ==tuple:\n",
    "        tokens =[item for sublist in tokens for item in sublist]  \n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in tokens if w not in stopwords) \n",
    "    mostCommon= allWordExceptStopDist.most_common(number)\n",
    "    top_ten_string = ' '.join([tup[0] for tup in mostCommon])\n",
    "    return top_ten_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run this on every column of the text \n",
    "top_ten = hdf.tokens.apply(top_tokens)\n",
    "#make a new column of the top tweets\n",
    "hdf['top_ten'] = top_ten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###merge sf tweets and counts with shape geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Retrieve the Shape Files for Each Block:\n",
    "geodf = pd.read_csv('../../data/intermediate_data/sf_only_sql_shapes.csv')\n",
    "#format the dataframe\n",
    "geodf['geoid10'] = geodf.geoid10.astype('str')\n",
    "geodf.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "#set the index as the geoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to alter the geoid10 column to merge with shape files\n",
    "hdf['geoid10'] =hdf['geoid10'].apply(lambda x: x[1:])\n",
    "#create a new dataframe \n",
    "hourlydf = pd.merge(geodf, hdf, on='geoid10', how='outer')\n",
    "#fill no tweets with a zero value\n",
    "hourlydf.tweetcnt.fillna(0, inplace = True)\n",
    "#drop empty hour columns\n",
    "hourlydf.dropna(subset = ['hour'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##make a different map for each hour! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_properties_geo(row):\n",
    "    geoid = row['geoid10']\n",
    "    tweetrate = row['tweetcnt']\n",
    "    top_ten = row['top_ten']\n",
    "    geo_json = {\"type\": \"Feature\", \"geometry\": json.loads(row['geometry']),  \"properties\": {'geoid': geoid ,'tweetrate': tweetrate, 'top_ten': top_ten }}\n",
    "    return geo_json\n",
    "\n",
    "def dataframe_to_geojson(df, outfilename):\n",
    "    '''Takes in a dataframe with a count, geoid10, and list of tokens. Dumps it into a json geojason file'''\n",
    "    df['geoid10'] = df['geoid10'].astype('str')\n",
    "    df[\"tweetcnt\"] = df['tweetcnt'].astype('str')\n",
    "    list_to_export = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_to_export.append(add_properties_geo(row))\n",
    "    with open(outfilename, 'w') as outfile:\n",
    "        json.dump(list_to_export, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###divide time into five segments for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hourlydf['hr_bin'] = pd.cut(hourlydf.hour, bins = 5, labels = ['latenight', 'dawn','morning','afternoon','evening'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/miniconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/christy/miniconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for time in hourlydf.hr_bin.unique():\n",
    "    time_df = hourlydf[hourlydf['hr_bin']== time]\n",
    "    output = 'data/' + time + '.json'\n",
    "    dataframe_to_geojson(time_df, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/miniconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/christy/miniconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dataframe_to_geojson(test_hour, 'data/testhour.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Map By Weekend/WeekDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = retrieve_and_merge_tweet_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###break the data into weekday/weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#goldengate=['060750601001000', '060750601001007'*, '060750601001016'\n",
    "crissyfield=['06075060100102', '060750601001052', '060750601001009']\n",
    "\n",
    "\n",
    "prosidiooffthegrid =['060750601001102']\n",
    "\n",
    "dolorespark= ['060750206001004', '06075026001009']\n",
    "\n",
    "pier= ['060750101001001', '060750101001001']\n",
    "\n",
    "remove= ['060750607001000', '060750179021003', '060750179021000', '060759804011003', '060750102001002', '060750102001000', '060759804011000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hour_df = mm.pipeline.transform_timestamp(df, DOW = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_df['tweetcnt'] = 1\n",
    "#get a total count of tweets\n",
    "hdf = hour_df.groupby(['geoid10', 'DOW']).agg(sum).reset_index().drop('id', 1)\n",
    "#get a grouped sum of the words\n",
    "hour_df_txt = hour_df.groupby(['geoid10', 'DOW'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "hdf['tokens'] = hour_df_txt['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###get weekend, weekdays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfweekend = hdf[hdf['DOW'] > 5].drop('DOW', 1)\n",
    "dfwnd = dfweekend.groupby('geoid10').agg(np.sum).reset_index()\n",
    "wknd_txt = dfweekend.groupby(['geoid10'])['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "dfwnd['tokens'] = wknd_txt['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>tweetcnt</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>19</td>\n",
       "      <td>[u'Acabo', u'de', u'publicar', u'una', u'foto'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001001</td>\n",
       "      <td>161</td>\n",
       "      <td>[u'a', u\"gentleman's\", u'quarters', u'at', u'#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001004</td>\n",
       "      <td>78</td>\n",
       "      <td>[u'BucketHats', u',', u'BreadBowls', u',', u'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001005</td>\n",
       "      <td>2</td>\n",
       "      <td>[u'Just', u'posted', u'a', u'photo', u'@', u\"P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001006</td>\n",
       "      <td>6</td>\n",
       "      <td>[u'Idk', u'how', u'many', u'shabby', u'ass', u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10  tweetcnt  \\\n",
       "0  060750101001000        19   \n",
       "1  060750101001001       161   \n",
       "2  060750101001004        78   \n",
       "3  060750101001005         2   \n",
       "4  060750101001006         6   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'Acabo', u'de', u'publicar', u'una', u'foto'...  \n",
       "1  [u'a', u\"gentleman's\", u'quarters', u'at', u'#...  \n",
       "2  [u'BucketHats', u',', u'BreadBowls', u',', u'a...  \n",
       "3  [u'Just', u'posted', u'a', u'photo', u'@', u\"P...  \n",
       "4  [u'Idk', u'how', u'many', u'shabby', u'ass', u...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwnd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweekday = hdf[hdf['DOW'] < 6].drop('DOW', 1)\n",
    "\n",
    "dfwkday = dfweekday.groupby('geoid10').agg(np.sum).reset_index()\n",
    "wkdy_txt = dfweekday.groupby('geoid10')['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "dfwkday['tokens'] = wkdy_txt['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>tweetcnt</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>52</td>\n",
       "      <td>[u'#Repost', u'@robtalcual', u'with', u'repost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001001</td>\n",
       "      <td>557</td>\n",
       "      <td>[u\"It's\", u'been', u'a', u'good', u'Sunday', u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001002</td>\n",
       "      <td>1</td>\n",
       "      <td>[u'Remember', u'why', u'you', u'started', u'ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001004</td>\n",
       "      <td>183</td>\n",
       "      <td>[u'My', u'last', u'#SFO', u'post', u'\\U0001f62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001005</td>\n",
       "      <td>9</td>\n",
       "      <td>[u'I', u'want', u'one', u'for', u'my', u'birth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10  tweetcnt  \\\n",
       "0  060750101001000        52   \n",
       "1  060750101001001       557   \n",
       "2  060750101001002         1   \n",
       "3  060750101001004       183   \n",
       "4  060750101001005         9   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'#Repost', u'@robtalcual', u'with', u'repost...  \n",
       "1  [u\"It's\", u'been', u'a', u'good', u'Sunday', u...  \n",
       "2  [u'Remember', u'why', u'you', u'started', u'ht...  \n",
       "3  [u'My', u'last', u'#SFO', u'post', u'\\U0001f62...  \n",
       "4  [u'I', u'want', u'one', u'for', u'my', u'birth...  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwkday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove the columns I want to remove \n",
    "dfweekend = dfweekend[~dfweekend['geoid10'].isin(remove)]\n",
    "dfwkday = dfwkday[~dfwkday['geoid10'].isin(remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run this on every column of the text \n",
    "top_ten_wknd = dfweekend.tokens.apply(top_tokens)\n",
    "#make a new column of the top tweets\n",
    "dfweekend['top_ten'] = top_ten_wknd\n",
    "\n",
    "top_ten_wkdy = dfwkday.tokens.apply(top_tokens)\n",
    "#make a new column of the top tweets\n",
    "dfwkday['top_ten'] = top_ten_wkdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###get shape geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Retrieve the Shape Files for Each Block:\n",
    "geodf = pd.read_csv('../../data/intermediate_data/sf_only_sql_shapes.csv')\n",
    "#format the dataframe\n",
    "geodf['geoid10'] = geodf.geoid10.astype('str')\n",
    "geodf.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "#set the index as the geoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwkday['geoid10'] =dfwkday['geoid10'].apply(lambda x: x[1:])\n",
    "#create a new dataframe \n",
    "dfwkday = pd.merge(geodf, dfwkday, on='geoid10', how='outer')\n",
    "#fill no tweets with a zero value\n",
    "dfwkday.tweetcnt.fillna(0, inplace = True)\n",
    "#drop empty hour columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweekend['geoid10'] =dfweekend['geoid10'].apply(lambda x: x[1:])\n",
    "#create a new dataframe \n",
    "dfweekend = pd.merge(geodf, dfweekend, on='geoid10', how='outer')\n",
    "#fill no tweets with a zero value\n",
    "dfweekend.tweetcnt.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###now make them a geojason!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_to_geojson(dfweekend, 'data/weekend.json')\n",
    "dataframe_to_geojson(dfwkday, 'data/weekday.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
