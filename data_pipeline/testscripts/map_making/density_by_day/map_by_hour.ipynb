{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import media_mapper as mm\n",
    "import nltk\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##gather data and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not sure if I should keep this \n",
    "def modify_tokens(df):\n",
    "    '''Tokens in dataframe were created by the library twokenize /\n",
    "    (https://github.com/myleott/ark-twokenize-py). This function modifes the tokens.\n",
    "    It also removed neighborhood blocks that are located in the ocean to clean\n",
    "    the data. The modifed dataframe is returned.\n",
    "    '''\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: literal_eval(x)[0])\n",
    "\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([p for p in punctuation])\n",
    "    stop.extend(['...',',,',',,,','..', 't','y','(@',')', 'c','i','I','a','…',\\\n",
    "                '@','.', 'co', 'com','amp', 'via','http','htt','https', '()',']'])\n",
    "    stop =[unicode(word) for word in stop]\n",
    "\n",
    "    #remove geoids located in the ocean \n",
    "    odd_ids = ['060750601001016', '060750179021003','060759901000003',\\\n",
    "               '060759901000002', '060750179021000','060750601001000',\\\n",
    "               '060759804011003', '060750201001001']\n",
    "    df = df[~df['geoid10'].isin(odd_ids)]\n",
    "    \n",
    "    #format tweets per hour \n",
    "    df['tph'] = df.tph.apply(lambda x: format(x, '.2f'))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_per_hour(df):\n",
    "    '''\n",
    "    INPUT: a dataframe with tweets tagged with time information.\n",
    "    OUTPUT: a transformed dataframe, where dataframe has been grouped\n",
    "        to obtain the rate of tweets per hour for each day.\n",
    "        Tokenized tweet text for every hour has been appended to one mater list.'''\n",
    "    \n",
    "    #set a count of tweets to determine tweet rate\n",
    "    df['tweetcnt'] = 1\n",
    "    #get a total count of tweets\n",
    "    dfh = df.groupby(['geoid10','date', 'hour']).agg(sum).reset_index().drop('id', 1)\n",
    "    #append the tokenized tweet data together\n",
    "    d_txt = df.groupby(['geoid10', 'date','hour'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "    #merge dataframes\n",
    "    dfh['tokens'] = d_txt['text']\n",
    "    dfh['tph'] = dfh['tweetcnt']\n",
    "    dfh.drop('tweetcnt', 1, inplace = True)\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###break data into hours and regions. add tweet corpus to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tweets_by_hour(df):\n",
    "    hdf = df.groupby(['geoid10', 'hour']).agg(np.mean).reset_index()\n",
    "    #get a grouped sum of the words\n",
    "    hour_df_txt = df.groupby(['geoid10', 'hour'])['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "    #merge these two dataframes together\n",
    "    hdf['tokens'] = hour_df_txt['tokens']\n",
    "    hdf['hr_bin'] = pd.cut(hdf.hour, bins = 5, labels = ['latenight', 'dawn','morning','afternoon','evening'])\n",
    "    return hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = mm.pipeline.retrieve_and_merge_tweet_data()\n",
    "df = mm.pipeline.transform_timestamp(df, hour = True)\n",
    "df = get_tweets_per_hour(df)\n",
    "df_hour = tweets_by_hour(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_hour = retrieve_geometry_information(df_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###merge sf tweets and counts with shape geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_geometry_information(df):\n",
    "    '''Obtains the geometry data for each geoid10. \n",
    "    Returns the dataframe with an extra geometry column.'''\n",
    "    ###Retrieve the Shape Files for Each Block:\n",
    "    geodf = pd.read_csv('../../../data/intermediate_data/sf_only_sql_shapes.csv')\n",
    "    #format the dataframe\n",
    "    geodf['geoid10'] = geodf.geoid10.astype('str')\n",
    "    geodf.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    #set the index as the geoid\n",
    "    #need to alter the geoid10 column to merge with shape files\n",
    "    \n",
    "    df['geoid10'] =df['geoid10'].apply(lambda x: x[1:])\n",
    "    #create a new dataframe \n",
    "    hourlydf = pd.merge(geodf, df, on='geoid10', how='outer')\n",
    "    #fill no tweets with a zero value\n",
    "    hourlydf.dropna(subset = ['hour'], inplace = True)\n",
    "    \n",
    "    return hourlydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['...',',,',',,,','..', 't','y','(@',')', 'c','i','I','a', ',',\\\n",
    "                '@','.', 'co', 'com','amp','?' 'via','http','htt','https', '()',']'])\n",
    "stopwords.extend([str(char) for char in punctuation])\n",
    "sstopwords=[unicode(word) for word in stopwords]\n",
    "\n",
    "def top_tokens(corpus_list, stopwords= stopwords, number=10):\n",
    "    '''Takes a list of tokens. Returns the top ten, unless a different number given.'''\n",
    "    #Checks to make sure the tokens are in a list, and not a string\n",
    "    tokens = literal_eval(corpus_list)\n",
    "    #If there are multiple tweets, flatten the list\n",
    "    if type(tokens) ==tuple:\n",
    "        tokens =[item for sublist in tokens for item in sublist]  \n",
    "    tokens = [re.sub(r'http.*$', '', item) for item in tokens]\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in tokens if w not in stopwords) \n",
    "    mostCommon= allWordExceptStopDist.most_common(number)\n",
    "    top_ten_string = ' '.join([tup[0] for tup in mostCommon])\n",
    "    return top_ten_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_properties_geo(row):\n",
    "    geoid = row['geoid10']\n",
    "    tweetrate = row['tph']\n",
    "    top_ten = row['top_ten']\n",
    "    geo_json = {\"type\": \"Feature\", \"geometry\": json.loads(row['geometry']), \\\n",
    "                \"properties\": {'geoid': geoid ,'tweetrate': tweetrate, 'top_ten': top_ten }}\n",
    "    return geo_json\n",
    "\n",
    "def dataframe_to_geojson(df, outfilename):\n",
    "    '''Takes in a dataframe with a count, geoid10, and list of tokens. Dumps it into a json geojason file'''\n",
    "    df['geoid10'] = df['geoid10'].astype('str')\n",
    "    df[\"tph\"] = df['tph'].astype('str')\n",
    "    list_to_export = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_to_export.append(add_properties_geo(row))\n",
    "    with open(outfilename, 'w') as outfile:\n",
    "        json.dump(list_to_export, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run this on every column of the text \n",
    "df_hour['top_ten'] = df_hour.tokens.apply(top_tokens)\n",
    "\n",
    "odd_ids = ['060750601001016', '060750179021003','060759901000003',\\\n",
    "               '060759901000002', '060750179021000','060750601001000',\\\n",
    "               '060759804011003', '060750201001001']\n",
    "    \n",
    "df_test = df_hour[~df_hour['geoid10'].isin(odd_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['latenight', 'dawn', 'afternoon', 'evening', 'morning'], dtype=object)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hour.hr_bin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latenight\n",
      "../../../../app/data/test_times/latenight.json\n",
      "dawn\n",
      "../../../../app/data/test_times/dawn.json\n",
      "afternoon\n",
      "../../../../app/data/test_times/afternoon.json\n",
      "evening\n",
      "../../../../app/data/test_times/evening.json\n",
      "morning\n",
      "../../../../app/data/test_times/morning.json\n"
     ]
    }
   ],
   "source": [
    "for time in df_hour.hr_bin.unique():\n",
    "    print time\n",
    "    df_hour = df_hour[df_hour['hr_bin']== time]\n",
    "    output = '../../../../app/data/test_times/' + time + '.json'\n",
    "    print output\n",
    "    dataframe_to_geojson(df_hour, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Map By Weekend/WeekDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###break the data into weekday/weekend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#goldengate=['060750601001000', '060750601001007'*, '060750601001016'\n",
    "crissyfield=['06075060100102', '060750601001052', '060750601001009']\n",
    "\n",
    "\n",
    "prosidiooffthegrid =['060750601001102']\n",
    "\n",
    "dolorespark= ['060750206001004', '06075026001009']\n",
    "\n",
    "pier= ['060750101001001', '060750101001001']\n",
    "\n",
    "remove= ['060750607001000', '060750179021003', '060750179021000', '060759804011003', '060750102001002', '060750102001000', '060759804011000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets_per_day(df):\n",
    "    '''\n",
    "    INPUT: a dataframe with tweets tagged with time information.\n",
    "    OUTPUT: a transformed dataframe, where dataframe has been grouped\n",
    "        to obtain the rate of tweets per hour for each day.\n",
    "        Tokenized tweet text for every hour has been appended to one mater list.'''\n",
    "    \n",
    "    #set a count of tweets to determine tweet rate\n",
    "    df['tweetcnt'] = 1\n",
    "    #get a total count of tweets\n",
    "    dfh = df.groupby(['geoid10','DOW', 'date']).agg(sum).reset_index().drop('id', 1)\n",
    "    #append the tokenized tweet data together\n",
    "    d_txt = df.groupby(['geoid10', 'DOW', 'date'])['text'].apply(lambda x: ','.join(x)).reset_index()\n",
    "    #merge dataframes\n",
    "    dfh['tokens'] = d_txt['text']\n",
    "    #tweetcnt is now the tweets per day (tpd)\n",
    "    dfh['tpd'] = dfh['tweetcnt']\n",
    "    dfh.drop('tweetcnt', 1, inplace = True)\n",
    "    return dfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = mm.pipeline.retrieve_and_merge_tweet_data()\n",
    "wkd_df = mm.pipeline.transform_timestamp(df, DOW = True)\n",
    "wkd_df = get_tweets_per_day(wkd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove geoids that are in the ocean\n",
    "odd_ids = ['060750601001016', '060750179021003','060759901000003',\\\n",
    "               '060759901000002', '060750179021000','060750601001000',\\\n",
    "               '060759804011003', '060750201001001']  \n",
    "wkd_df = wkd_df[~wkd_df['geoid10'].isin(odd_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "wknd_df = wkd_df.groupby(['geoid10', 'DOW']).agg(np.mean).reset_index()\n",
    "#get a grouped sum of the words\n",
    "wkd_df_txt = wkd_df.groupby(['geoid10', 'DOW'])['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "wknd_df['tokens'] = wkd_df_txt['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>DOW</th>\n",
       "      <th>tpd</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[u'#Repost', u'@robtalcual', u'with', u'repost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[u'Dinner', u'in', u'San', u'Fran', u'with', u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>[u'@scomassf', u'http://t.co/WnccwChDbQ', u'We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[u'*', u'crab', u'emoji', u'*', u'@', u'Pier',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[u'The', u'#rough', u'#wild', u'#choppy', u'#p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10  DOW  tpd  \\\n",
       "0  060750101001000    0  4.0   \n",
       "1  060750101001000    1  2.5   \n",
       "2  060750101001000    2  2.5   \n",
       "3  060750101001000    3  4.0   \n",
       "4  060750101001000    4  7.0   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'#Repost', u'@robtalcual', u'with', u'repost...  \n",
       "1  [u'Dinner', u'in', u'San', u'Fran', u'with', u...  \n",
       "2  [u'@scomassf', u'http://t.co/WnccwChDbQ', u'We...  \n",
       "3  [u'*', u'crab', u'emoji', u'*', u'@', u'Pier',...  \n",
       "4  [u'The', u'#rough', u'#wild', u'#choppy', u'#p...  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wknd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###get weekend, weekdays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weeknd_df = wknd_df[wknd_df['DOW'] > 5].drop('DOW', 1)\n",
    "df_weeknd = weeknd_df.groupby('geoid10').agg(np.mean).reset_index()\n",
    "wknd_txt = weeknd_df.groupby(['geoid10'])['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "df_weeknd['tokens'] = wknd_txt['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>tpd</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>[u'Acabo', u'de', u'publicar', u'una', u'foto'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001001</td>\n",
       "      <td>53.666667</td>\n",
       "      <td>[u'a', u\"gentleman's\", u'quarters', u'at', u'#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001004</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>[u'BucketHats', u',', u'BreadBowls', u',', u'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[u'Just', u'posted', u'a', u'photo', u'@', u\"P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001006</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>[u'Idk', u'how', u'many', u'shabby', u'ass', u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10        tpd  \\\n",
       "0  060750101001000   6.333333   \n",
       "1  060750101001001  53.666667   \n",
       "2  060750101001004  26.000000   \n",
       "3  060750101001005   1.000000   \n",
       "4  060750101001006   2.000000   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'Acabo', u'de', u'publicar', u'una', u'foto'...  \n",
       "1  [u'a', u\"gentleman's\", u'quarters', u'at', u'#...  \n",
       "2  [u'BucketHats', u',', u'BreadBowls', u',', u'a...  \n",
       "3  [u'Just', u'posted', u'a', u'photo', u'@', u\"P...  \n",
       "4  [u'Idk', u'how', u'many', u'shabby', u'ass', u...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weeknd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfweekday = wknd_df[wknd_df['DOW'] < 6].drop('DOW', 1)\n",
    "\n",
    "dfwkday = dfweekday.groupby('geoid10').agg(np.mean).reset_index()\n",
    "wkdy_txt = dfweekday.groupby('geoid10')['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#merge these two dataframes together\n",
    "dfwkday['tokens'] = wkdy_txt['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geoid10</th>\n",
       "      <th>tpd</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>060750101001000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>[u'#Repost', u'@robtalcual', u'with', u'repost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>060750101001001</td>\n",
       "      <td>40.194444</td>\n",
       "      <td>[u\"It's\", u'been', u'a', u'good', u'Sunday', u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>060750101001002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[u'Remember', u'why', u'you', u'started', u'ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>060750101001004</td>\n",
       "      <td>13.416667</td>\n",
       "      <td>[u'My', u'last', u'#SFO', u'post', u'\\U0001f62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>060750101001005</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>[u'I', u'want', u'one', u'for', u'my', u'birth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           geoid10        tpd  \\\n",
       "0  060750101001000   4.666667   \n",
       "1  060750101001001  40.194444   \n",
       "2  060750101001002   1.000000   \n",
       "3  060750101001004  13.416667   \n",
       "4  060750101001005   1.600000   \n",
       "\n",
       "                                              tokens  \n",
       "0  [u'#Repost', u'@robtalcual', u'with', u'repost...  \n",
       "1  [u\"It's\", u'been', u'a', u'good', u'Sunday', u...  \n",
       "2  [u'Remember', u'why', u'you', u'started', u'ht...  \n",
       "3  [u'My', u'last', u'#SFO', u'post', u'\\U0001f62...  \n",
       "4  [u'I', u'want', u'one', u'for', u'my', u'birth...  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwkday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seperate_weekends(df, weekend):\n",
    "    '''Takes a dataframe with a column marked with the day of week.\n",
    "    If weekend is True, returns a dataframe with just the weekend values\n",
    "    If false, returns a dataframe with just the weekday values.\n",
    "    Performs groupby to get mean tweet per day based on this grouping.'''\n",
    "    if weekend == True:\n",
    "        daysofweek = [6,7]\n",
    "    else:\n",
    "        threshold = [1,2,3,4,5]\n",
    "    df = df[df['DOW'].isin(daysofweek)].drop('DOW', 1)\n",
    "    dfweek = df.groupby('geoid10').agg(np.mean).reset_index()\n",
    "    dfweek_txt = df.groupby('geoid10')['tokens'].apply(lambda x: ','.join(x)).reset_index()\n",
    "    #merge these two dataframes together\n",
    "    dfweek['tokens'] = dfweek_txt['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run this on every column of the text \n",
    "dfwkday['top_ten'] = dfwkday.tokens.apply(top_tokens)\n",
    "#make a new column of the top tweets\n",
    "df_weeknd['top_ten'] = df_weeknd.tokens.apply(top_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_geometry_information(df):\n",
    "    '''Obtains the geometry data for each geoid10. \n",
    "    Returns the dataframe with an extra geometry column.'''\n",
    "    ###Retrieve the Shape Files for Each Block:\n",
    "    geodf = pd.read_csv('../../../data/intermediate_data/sf_only_sql_shapes.csv')\n",
    "    #format the dataframe\n",
    "    geodf['geoid10'] = geodf.geoid10.astype('str')\n",
    "    geodf.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    #set the index as the geoid\n",
    "    #need to alter the geoid10 column to merge with shape files\n",
    "    \n",
    "    df['geoid10'] =df['geoid10'].apply(lambda x: x[1:])\n",
    "    #create a new dataframe \n",
    "    weekdf = pd.merge(geodf, df, on='geoid10', how='outer')\n",
    "    #fill no tweets with a zero value\n",
    "    weekdf.dropna(subset = ['tpd'], inplace = True)\n",
    "    return weekdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dfwkday['geoid10'] = df_weeknd.geoid10.apply(lambda x: str(6) + x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###get shape geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_end = retrieve_geometry_information(df_weeknd)\n",
    "df_day = retrieve_geometry_information(dfwkday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>geoid10</th>\n",
       "      <th>tpd</th>\n",
       "      <th>tokens</th>\n",
       "      <th>top_ten</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....</td>\n",
       "      <td>60750179021023</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>[u'Not', u'a', u'bad', u'view', u'on', u'my', ...</td>\n",
       "      <td>bridge bay san wherever #sanfran shopping tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....</td>\n",
       "      <td>60750179021008</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>[u'Two', u'lanes', u'blocked', u'in', u'#BayBr...</td>\n",
       "      <td>buena yerba back mins lanes island two eb dela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....</td>\n",
       "      <td>60750179021057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[u'raw', u'n', u'true']</td>\n",
       "      <td>raw true n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....</td>\n",
       "      <td>60750179021054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[u'@chrisjrn', u'wait', u'what', u'were', u'yo...</td>\n",
       "      <td>@chrisjrn thinking wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....</td>\n",
       "      <td>60750179021060</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[u'The', u'cool', u',', u'grey', u'city', u'of...</td>\n",
       "      <td>bridge bay city … treasure love bottling isla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             geometry         geoid10  \\\n",
       "5   {\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....  60750179021023   \n",
       "6   {\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....  60750179021008   \n",
       "14  {\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....  60750179021057   \n",
       "19  {\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....  60750179021054   \n",
       "26  {\"type\":\"MultiPolygon\",\"coordinates\":[[[[-122....  60750179021060   \n",
       "\n",
       "         tpd                                             tokens  \\\n",
       "5   1.333333  [u'Not', u'a', u'bad', u'view', u'on', u'my', ...   \n",
       "6   2.000000  [u'Two', u'lanes', u'blocked', u'in', u'#BayBr...   \n",
       "14  1.000000                            [u'raw', u'n', u'true']   \n",
       "19  1.000000  [u'@chrisjrn', u'wait', u'what', u'were', u'yo...   \n",
       "26  1.000000  [u'The', u'cool', u',', u'grey', u'city', u'of...   \n",
       "\n",
       "                                              top_ten  \n",
       "5    bridge bay san wherever #sanfran shopping tri...  \n",
       "6   buena yerba back mins lanes island two eb dela...  \n",
       "14                                         raw true n  \n",
       "19                            @chrisjrn thinking wait  \n",
       "26   bridge bay city … treasure love bottling isla...  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###now make them a geojason!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_to_geojson(df_end, '../../../../app/data/test_times/weekend.json')\n",
    "dataframe_to_geojson(df_day, '../../../../app/data/test_times/weekday.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_properties_geo(row):\n",
    "    '''Translates a row of a dataframe into a geo_json string'''\n",
    "\n",
    "    geoid = row['geoid10']\n",
    "    tweetrate = row['tpd']\n",
    "    top_ten = row['top_ten']\n",
    "    geo_json = {\"type\": \"Feature\", \"geometry\": json.loads(row['geometry']), \\\n",
    "                \"properties\": {'geoid': geoid ,'tweetrate': tweetrate, 'top_ten': top_ten }}\n",
    "    return geo_json\n",
    "\n",
    "def dataframe_to_geojson(df, outfilename):\n",
    "    '''Takes in a dataframe with a count, geoid10, and list of tokens. \n",
    "    Dumps it into a geojson file for mapping.ß'''\n",
    "    \n",
    "    df['geoid10'] = df['geoid10'].astype('str')\n",
    "    df[\"tpd\"] = df['tpd'].astype('str')\n",
    "    list_to_export = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_to_export.append(add_properties_geo(row))\n",
    "    with open(outfilename, 'w') as outfile:\n",
    "        json.dump(list_to_export, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
