import pandas as pd
import json
import requests
import re
import psycopg2
import glob
import time
from sqlalchemy import create_engine

'''Creates a database in PostgreSQL for storing twitter json files generated by 
twitter_scraper.py. Also uses shape data for San Francisco blocks to determine 
which block each tweet occured in off of latitude/longitude data. '''

#READ TWITTER JSON FILES FROM FOLDER:

def json_to_df(filename, columns = None):
    '''Takes in a json file (such as tweets). Returns a pandas dataframe'''
    with open(filename, 'r') as f:
        l = f.readlines()
    #some files may be empty, allows code to still run in those cases.
    try:
        data = [json.loads(s) for s in l]
    except Exception as e:
        print e, filename
        data = None
    if columns:
        df = pd.DataFrame(data)[columns]
    else:
        df = pd.DataFrame(data)
    return df

def get_mega_dataframe(filepath, columns = ['coordinates', 'text', 'timestamp_ms', 'id']):
    '''Reads a directory of json files, and loads them into one pandas dataframe. 
    Calls the function json_to_df() to read each json file.     

    PARAMETERS: 
    filepath - a directory of json twitterdata.
    columns- column labels to extract for the dataframe. 
    
    OUTPUT: 
    A pandas dataframe consisting of all extracted json data from all tweets.
    '''
    #create a list of every json file in a directory
    tweet_files= glob.glob(filepath)
    #read files into a list of dataframes 
    df_list = [json_to_df(tf, columns=columns) for tf in tweet_files]
    #remove empty dataframes
    df_list = [df for df in df_list if df is not None]
    df = pd.concat(df_list).reset_index(drop = True)
    
    return df

#CLEAN TWITTER DATA IN DATAFRAMES TO INSERT INTO DATABASE

def extract_columns(df):
    '''Extracts latitude and longitude from the coordinates entry. 

    PARAMETERS: A pandas dataframe. 
    OUTPUT: A smaller dataframe containing the columns 
            'id', 'text, coordinates, timestamp' '''
    #only include rows with coordinates
    df = df[~df['coordinates'].isnull()]
    #make a new dataframe with coordinates, tweets, and timestamps
    df = df[['id', 'text, coordinates, timestamp']]
    #get a list of coordinates to break it into longitude and latitude data
    coor = df.coordinates.tolist()
    #list of the longitude coordinates
    lons = [c['coordinates'][0] for c in coor]
    #list of the latitude coordinates 
    lats = [c['coordinates'][1] for c in coor] 
    #turn lats and longs into panda series. Append them to the dataframe.
    df['lons'] = pd.Series(lons)
    df['lats'] = pd.Series(lats)
    df = df.drop('coordinates', 1)   
    df = df.drop_duplicates()
    return df 

#PLACE TWITTER DATA IN POSTGRESQL. 

def create_tweet_table_sql(df, tablename='tweets', remove_text_column = True):
    ''' 
    PARAMETERS: 
    df - Pandas dataframe to place in PostgreSQL. 
    tablename -  Name of table to create or add data to in PostgreSQL
    remove_text_column -  Boolean. 
        If true, returns a dataframe with a 'text' and 'id' column.
        These contain the tweet text, and unique id for each tweet.
        If false, tweet text is included in the PostgreSQL database. 
        Tweet text is first stripped of unicode for compatibility. 
    
    OUTPUT: 
    Creates or appends a table 'tablename' in PostgreSQL with the information
    in df.  
    If remove_test_column is True, the text column is removed from the table, 
    and dataframe is returned containing the tweet text, and tweet id.    '''
    
    if remove_text_column == True:
        #create a new dataframe with tweet text and id. 
        tweettext_df = df[['text', 'id']]
        #remove text so that it is not passed to PostgreSQL. 
        df.pop('text')
        engine = create_engine('postgresql://clwilloughby:christy@localhost:5432/zipfiantwitter')
        #place dataframe into PostgreSQL database
        df.to_sql(tablename, engine, if_exists='replace')
        #return a table of tweet text, and tweet id
        return tweettext_df
    
    else:
        #clean text so its compatible with PostgreSQL
        df['text'] = [re.sub('[^A-Za-z0-9]+', ' ',s)for s in df.text.tolist()]
        #create a table in PostgreSQL containing the entire dataframe. 
        engine = create_engine('postgresql://clwilloughby:christy@localhost:5432/zipfiantwitter')
        df.to_sql(tablename, engine, if_exists='replace')

    #save a copy of the dataframe to doublecheck formatting later 
    df_alltweets.to_pickle('data/intermediate_data/jasontweets_in_df_pre.pkl')

def format_tweet_table_sql(tweet_table='tweets' , \
                        new_geo_tweet_table = 'tweets_with_geo',\
                         census_table = 'sf_neighb'):
    '''
    Uses latitude and longitude provided in the tweet table, and the block information
    from the table of san francisco blocks (sf_neighb) to deduce the geoid of the block 
    each tweet occured in. Filters out tweets that are not in SF county.

    PARAMETERS:
    tweet_table - a PostgreSQL table of tweets (generated in create_tweet_table_sql()).
    new_geo_tweet_table - a new PostgreSQL 
    census_table - a PostgreSQL table ontaining the shape files for the US Census blocks

    OURPUT:  A new table is generated in PostgreSQL (new_geo_tweet_table). 
    Table contains the time and geometry data for tweets only in sanfrancisco '''

    #connect to PostgreSQL database
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()

    #add a geometry column to the tweet table
    query = """ SELECT AddGeometryColumn(%s,'geom',4326,'POINT',2);""" %tweet_table
    c.execute(query)
    conn.commit()

    #use latitude and longitude of tweets to determine the block each tweet occured in
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """UPDATE %s SET geom = ST_SetSRID(ST_MakePoint(lons, lats), 4326);""" %tweet_table 
    c.execute(query)
    conn.commit()            

    #reformat the table of san francisco neighborhood blocks
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """SELECT UpdateGeometrySRID(%s, 'wkb_geometry', 4326);""" %census_table
    c.execute(query)
    conn.commit() 

    #create a new table which contains all tweets in san francisco, as well as the
    #geoid and geometry coordinates for the neighborhood block. 
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """ SELECT points.*, polys.geoid10
                    INTO %s 
                    FROM %s polys
                    JOIN %s points 
                    ON (ST_Within(points.geom,polys.wkb_geometry) AND
                    polys.countyfp10 = '075');""" %(new_geo_tweet_table, census_table, tweet_table)
    c.execute(query)
    conn.commit()
    conn.close() 

if __main__ == '__name__':
    #call tweets from a filepath: 	
    df = get_mega_dataframe('../../../../../Desktop/toomanytweets/*json')

    #extract meaningful info from the columns:
    df = extract_columns(df)
    
    #place dataframe into PostgreSQL
    df = create_tweet_table_sql(df)

    #tokenize tweet text, and save it to a csv.
    mm.pipeline.twokenize_text(df)
    
    #use latitude and longitude to determine what 
    #neighborhood block each tweet longs to
    format_tweet_table_sql()
