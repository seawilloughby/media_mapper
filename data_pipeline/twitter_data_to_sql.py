import pandas as pd
import json
import requests
import re
import psycopg2
import glob
import time
from sqlalchemy import create_engine



'''Creates a database in postgress for storing twitter json files generated by twitter_scraper.py
 and San Francisco block geography shape file. '''



#READ TWITTER JSON FILES FROM FOLDER:

def json_to_df(filename, columns = None):
    '''Takes in a json file (such as tweets). Returns a pandas dataframe'''
    with open(filename, 'r') as f:
        l = f.readlines()
    try:
        data = [json.loads(s) for s in l]
    except Exception as e:
        print e, filename
        data = None
    if columns:
        df = pd.DataFrame(data)[columns]
    else:
        df = pd.DataFrame(data)
    return df

def get_mega_dataframe(filepath, columns = ['coordinates', 'text', 'timestamp_ms', 'id']):
    '''Reads a directory of json files into one pandas dataframe.

    INPUT: filepath - a directory json twitterdata.
    OUTPUT: a pandas dataframe
    '''
    #create a list of every json file in a directory
    tweet_files= glob.glob(filepath)
    #read files into a list of dataframes 
    df_list = [json_to_df(tf, columns=columns) for tf in tweet_files]
    #remove empty dataframes
    df_list = [df for df in df_list if df is not None]
    df = pd.concat(df_list).reset_index(drop = True)
    
    return df

#CLEAN TWITTER DATA IN DATAFRAMES TO INSERT INTO DATABASE

def extract_columns(df):
    '''Called by clean_text_for_sql. Takes in a pandas dataframe. Returns a smaller dataframe:
    Text, Coordinates, timestamp'''
    #only include rows with coordinates
    df = df[~df['coordinates'].isnull()]
    #make a new dataframe with coordinates, tweets, and timestamps
    df = df[['coordinates', 'text' ,'timestamp_ms']]
    #get a list of coordinates to break it into long and lat data
    coor = df.coordinates.tolist()
    #list of the longitude coordinates
    lons = [c['coordinates'][0] for c in coor]
    #list of the latitude coordinates 
    lats = [c['coordinates'][1] for c in coor] 
    #turn lats and longs into panda series. Append them to the dataframe.
    df['lons'] = pd.Series(lons)
    df['lats'] = pd.Series(lats)
    df = df.drop('coordinates', 1)   
    return df 

def clean_text_for_sql(df):
    '''Takes in a dataframe with a text column containing emoticons, ect. 
    Returns a dataframe where the text has been striped of punctuation and repeats
    Also reorders the columns to fit the order I want for SQL'''
    df = extract_columns(df)
    #remove unicode and punctuation to make text compatible with sql
    df['text'] = [re.sub('[^A-Za-z0-9]+', ' ',s)for s in df.text.tolist()]
    df.columns.tolist()          
    #select the columns to input into sql
    ordered_colums = ['timestamp_ms','text', 'lats', 'lons' ]
    df = df[ordered_colums]
    #drop any replicate rows
    df = df.drop_duplicates()
    return df
    

#PLACE TWITTER DATA IN SQL. USE TO GET GEO COORDINATES

def create_tweet_table_sql(df, tablename='tweetsv4', remove_text_column = True):
    ''' 
    INPUT: dataframe to place in sql. 
    
    OUPUT: creates a sql table give by the argument 'tabelname'.
    The text column is not used in postgress data manipulation. 
    If remove_test_column is True, the text column is removed from the table, 
    and dataframe is returned containing the tweet text, and tweet id.    '''
    

    if remove_text_column == True:
        #create a new dataframe with tweet text and id. 
        tweettext_df = df[['text', 'id']]
        #remove text so that it is not passed to postgress. 
        df.pop('text')
        engine = create_engine('postgresql://clwilloughby:christy@localhost:5432/zipfiantwitter')
        #place dataframe into postgress database
        df.to_sql(tablename, engine, if_exists='replace')
        #return a table of tweet text, and tweet id
        return tweettext_df
    
    else:
        #create a table in postgress containing the entire dataframe. 
        engine = create_engine('postgresql://clwilloughby:christy@localhost:5432/zipfiantwitter')
        df.to_sql(tablename, engine, if_exists='replace')

    #save a copy of the dataframe to doublecheck formatting later 
    df_alltweets.to_pickle('data/intermediate_data/jasontweets_in_df_pre.pkl')

def format_tweet_table_sql(tweet_table='tweetsv4' , new_geo_tweet_table = 'tweets_with_geo'):
    '''
    INPUT: a postgress table of tweets (generated in create_tweet_table_sql).

    Uses latitude and longitude provided in the tweet table, and the block information
    from the table of san francisco blocks to deduce the geoid of the block each tweet occured in. 
    Filters out tweets that are not in SF county.

    OURPUT:  A new table is generated in postgress (new_geo_tweet_table). Table contains the time
    and geometry data for tweets only in sanfrancisco '''

    #connect to postgress database
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()

    #add a geometry column to the tweet table
    query = """ SELECT AddGeometryColumn(%s,'geom',4326,'POINT',2);""" %tweet_table
    c.execute(query)
    conn.commit()

    #use latitude and longitude of tweets to determine the block each tweet occured in
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """UPDATE %s SET geom = ST_SetSRID(ST_MakePoint(lons, lats), 4326);""" %tweet_table 
    c.execute(query)
    conn.commit()            

    #reformat the table of san francisco neighborhood blocks
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """SELECT UpdateGeometrySRID('sf_neighb', 'wkb_geometry', 4326);""" 
    c.execute(query)
    conn.commit() 

    #create a new table which contains all tweets in san francisco, as well as the
    #geoid and geometry coordinates for the neighborhood block. 
    conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')
    c = conn.cursor()
    query = """ SELECT points.*, polys.geoid10
                    INTO %s 
                    FROM sf_neighb polys
                    JOIN %s points 
                    ON (ST_Within(points.geom,polys.wkb_geometry) AND
                    polys.countyfp10 = '075');""" %(new_geo_tweet_table, tweet_table)
    c.execute(query)
    conn.commit()
    conn.close()
 

if __main__ == '__name__':
	#call tweets from a filepath: 	
	df = get_mega_dataframe('../../../../../Desktop/toomanytweets/*json')
	
	#extract meaningful info from the columns:
	df = clean_text_for_sql(df)
    
    #place dataframe into postgress
    create_tweet_table_sql(df)

    #use latitude and longitude to determine what 
    #neighborhood block each tweet longs to
    format_tweet_table_sql()
