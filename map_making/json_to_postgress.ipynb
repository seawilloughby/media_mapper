{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import psycopg2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Get Data From JSON File. Put Into Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Import Data\n",
    "notes to self: if file is empty, or json object is incomplete, it may get mad. What can I do about this?\n",
    "\n",
    "A. ONE FILE\n",
    "\n",
    "B. ALL FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all the tweet files from one week\n",
    "tweet_files= glob.glob('../../../misc/twitter_data/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unterminated string starting at: line 1 column 764 (char 763)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0f287fd811df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mjson_data_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mega_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../misc/twitter_data/*.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-0f287fd811df>\u001b[0m in \u001b[0;36mget_mega_dataframe\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ad4234d015cf>\u001b[0m in \u001b[0;36mjson_to_df\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christy/miniconda/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christy/miniconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/christy/miniconda/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \"\"\"\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No JSON object could be decoded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unterminated string starting at: line 1 column 764 (char 763)"
     ]
    }
   ],
   "source": [
    "def get_mega_dataframe(filepath):\n",
    "    '''Takes a filepath to a directory of json files for twitterdata\n",
    "    Runs through all the files, reads them into a pandas dataframe, \n",
    "    and returns a mega-dataframe of all the json files appended together'''\n",
    "    tweet_files= glob.glob(filepath)\n",
    "    df = json_to_df(tweet_files[0])\n",
    "    for tf in tweet_files[1:]:\n",
    "        df_new = json_to_df(tf)\n",
    "        df.append(df_new)\n",
    "    return df\n",
    "\n",
    "df_tweets = get_mega_dataframe('../../../misc/twitter_data/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def json_to_df(filename):\n",
    "    '''Takes in a json file. Returns a pandas dataframe'''\n",
    "    with open(filename, 'r') as f:\n",
    "        l = f.readlines()\n",
    "\n",
    "    data = [json.loads(s) for s in l]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_tweets = json_to_df(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2. Format Data to Put in Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_columns(df):\n",
    "    '''Called by clean_text_for_sql. Takes in a pandas dataframe. Returns a smaller dataframe:\n",
    "    Text, Coordinates, timestamp'''\n",
    "    #only include rows with coordinates\n",
    "    df = df[~df['coordinates'].isnull()]\n",
    "    #make a new dataframe with coordinates, tweets, and timestamps\n",
    "    df = df[['coordinates', 'text' ,'timestamp_ms']]\n",
    "    #get a list of coordinates to break it into long and lat data\n",
    "    coor = df.coordinates.tolist()\n",
    "    #list of the longitude coordinates\n",
    "    lons = [c['coordinates'][0] for c in coor]\n",
    "    #list of the latitude coordinates \n",
    "    lats = [c['coordinates'][1] for c in coor]\n",
    "    \n",
    "    #turn lats and longs into panda series. Append them to the dataframe.\n",
    "    df['lons'] = pd.Series(lons)\n",
    "    df['lats'] = pd.Series(lats)\n",
    "    df = df.drop('coordinates', 1)   \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text_for_sql(df):\n",
    "    '''Takes in a dataframe with a text column containing emoticons, ect. \n",
    "    Returns a dataframe where the text has been striped of punctuation and repeats\n",
    "    Also reorders the columns to fit the order I want for SQL'''\n",
    "    df = extract_columns(df)\n",
    "    df['text'] = [re.sub('[^A-Za-z0-9]+', ' ',s)for s in df.text.tolist()]\n",
    "    df.columns.tolist()          \n",
    "    ordered_colums = [u'timestamp_ms',u'text', 'lats', 'lons' ]\n",
    "    return df[ordered_colums]\n",
    "\n",
    "\n",
    "df_ordered = clean_text_for_sql(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3. Export The DF Into a  SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'timestamp_ms', u'text', u'lats', u'lons'], dtype='object')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ordered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop rows if there are repeats \n",
    "df_twitter = df_ordered.drop_duplicates(subset='timestamp_ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56663, 4)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_twitter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_twitter.to_csv('data_for_sql.csv', encoding = 'utf-8', index = False, header=False, if_exists ='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Alternative method: Use SQLalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://clwilloughby:christy@localhost:5432/zipfiantwitter')\n",
    "df_twitter.to_sql(\"tweetedBIG\", engine, if_exists='replace')\n",
    "#when I do this, NOW THE THING HAS TO BE IN QUOTES. OOPS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FORMAT THE CSV IN SQL TO GET COORDINATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"\"\" SELECT AddGeometryColumn('tweetedBIG','geom',4326,'POINT',2);\"\"\"\n",
    "c.execute(query)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')\n",
    "c = conn.cursor()\n",
    "query = \"\"\"UPDATE \"tweetedBIG\" SET geom = ST_SetSRID(ST_MakePoint(lons, lats), 4326);\"\"\"\n",
    "c.execute(query)\n",
    "conn.commit()            \n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')\n",
    "c = conn.cursor()\n",
    "query = \"\"\"SELECT UpdateGeometrySRID('sf_neighb', 'wkb_geometry', 4326);\"\"\" \n",
    "c.execute(query)\n",
    "conn.commit() \n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')\n",
    "c = conn.cursor()\n",
    "query = \"\"\" SELECT points.*, polys.geoid10\n",
    "                INTO tweeted_neighb_BIG \n",
    "                FROM sf_neighb polys\n",
    "                JOIN \"tweetedBIG\" points \n",
    "                ON ST_Within(points.geom,polys.wkb_geometry);\"\"\"\n",
    "c.execute(query)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Also I can use Psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "conn = psycopg2.connect(dbname='zipfiantwitter', user ='clwilloughby', host = '/tmp')\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.rollback()  #roll back all changes done to the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute('''SELECT * \n",
    "    FROM tweeted\n",
    "    LIMIT 10;''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''CREATE TABLE sf_tweets AS\n",
    "    SELECT *\n",
    "    FROM tweeted\n",
    "    WHERE logins.tmstmp > current_date - 7\n",
    "    GROUP BY userid;''' % timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.commit()\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Option 2: Send it to a CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Commands entered in postgress\n",
    "\n",
    "```psql\n",
    "CREATE DATABASE zipfiantwitter\n",
    "\\l```\n",
    "\n",
    "Things became obnoxious and i changed things:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE tweet(\n",
    "    timestamp_ms bigint,\n",
    "    text varchar(255),\n",
    "    lats FLOAT,\n",
    "    lons FLOAT\n",
    "    );\n",
    "    \n",
    "COPY tweeting FROM '/Users/clwilloughby/Documents/root/repos/media_mapper/map_tests/postgress_exp/data_for_sql.csv' WITH (FORMAT CSV, DELIMETER ',');\n",
    "```\n",
    "   ```\n",
    "   \n",
    "If I need to change the column type:\n",
    "```sql\n",
    "ALTER COLUMN timestamp_ms SET DATA TYPE bigint ;\n",
    "ALTER COLUMN presales TYPE numeric\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
